17.06 Инжиниринг данных: экзамен

# Тема

Автоматизация и оркестрация пайплайна машинного обучения с использованием:
- Apache Airflow и 
- облачного хранилища.

# Описание задания

Вы инженер данных в медицинском центре, где разрабатываются предиктивные модели диагностики заболеваний. Ваша задача — спроектировать и реализовать автоматизированный ETL-процесс: от получения медицинских данных до выгрузки результатов модели в облачное хранилище с помощью Apache Airflow и Python. 
Результат работы: воспроизводимый проект в виде репозитория с пояснениями в формате README.

# Описание данных

Датасет: Breast Cancer Wisconsin Diagnostic (в формате CSV). Загрузите его из источника.

Структура файлов и папок репозитория:
```text 
dags/ — DAG-файл для Airflow.
etl/ — модульные Python-скрипты для шагов пайплайна (загрузка, предобработка, обучение, метрики).
results/ — финальные метрики и сохраненные артефакты модели.
logs/ — лог-файлы (если есть).
README.md — документация проекта (включая описание архитектуры, инструкции, анализ отказов и идей по улучшению).
```

# Алгоритм выполнения задания

## Этап 1. Планирование пайплайна (описание — в README)

- Изучите датасет и опишите, какую задачу машинного обучения вы решаете.
- Опишите структуру пайплайна: этапы обработки, связи между ними.
- Создайте схему пайплайна (блок-схему или диаграмму) и добавьте ее в README (в виде изображения или псевдокода).
- README-файл должен содержать:
  - формулировку ML-задачи;
  - схематичное представление пайплайна;
  - краткое описание шагов.

## Этап 2. Разработка ETL-компонентов (исходный код — в папке etl/)

Реализуйте отдельные Python-скрипты или функции:
- для загрузки и первичного анализа данных;
- очистки и предобработки, например, переименование колонок, удаление лишнего, нормализация;
- обучения модели LogisticRegression;
- расчета метрик (Accuracy, Precision, Recall, F1);
- сохранения результатов (в формате JSON или CSV).

> Важно! Код должен быть воспроизводимым: 
> - запускаться как из Airflow, так и отдельно. 
> - Стоит предусмотреть передачу параметров через аргументы или конфигурационные файлы. 
> - README-файл должен содержать:
>   - назначение каждого скрипта и их связи внутри пайплайна.

## Этап 3. Оркестрация пайплайна с помощью Airflow (файл — в dags/)

- Создайте DAG-файл (например, pipeline_dag.py), где 
каждый этап реализован как отдельный PythonOperator или BashOperator.
- Задайте зависимости между задачами в соответствии с логикой пайплайна.
- Обеспечьте логирование (в папку logs/) и настройте интервал (раз в день, по умолчанию — без расписания).

README-файл должен содержать:
- название DAG;
- описание зависимостей между задачами (текстом);
- инструкцию по запуску DAG (например, airflow tasks test task_id run_date).

## Этап 4. Интеграция с облачным хранилищем или локальным диском

Выгрузите результаты пайплайна (модель и метрики):
- либо в облачное хранилище (Google Drive API, Dropbox API, Amazon S3-compatible);
- либо на локальный диск (в подкаталог results/).

Реализуйте авторизацию (через credentials.json, OAuth-токен или переменные окружения) и отправку файла.

README-файл должен содержать:
- описание интеграции (как работает, что подключается);
- место хранения ключей;
- формат данных и пример запроса/ответа или пример кода.
- Если используете локальный диск, поясните структуру хранения, формат и логику использования.

## Этап 5. Анализ ошибок и устойчивости

Опишите в README потенциальные точки сбоя в пайплайне, ответив на вопросы:
- Где может «упасть» процесс?
- Какие исключения могут возникнуть?
- Что произойдет при потере соединения с источником данных?
- Что будет, если источник отдает невалидные данные?
- Что произойдет, если модель не обучается или выдает ошибку?

Важно продемонстрировать, 
- насколько ваш проект устойчив (Robustness), то есть способен продолжать работу при нештатных ситуациях. 
- Сбой в одной задаче не должен тянуть за собой весь пайплайн. 
- Покажите, что вы предусмотрели следующие моменты:
  - наличие логирования на каждом шаге;
  - использование Airflow retries, timeout, failure callbacks;
  - отдельные модули с изолированным функционированием.

Например:
- Источник данных (API) может быть недоступен →
 реализована повторная попытка соединения (retry в Airflow).
- Сырые данные могут содержать null-значения или неожиданную структуру →
 реализованы проверки схемы и валидация и т. д.

# Требования к выполненной работе

Вам предстоит сделать проект в виде репозитория с исходным кодом в README.

README.md — это основной отчет по выполнению задания. Он должен содержать:
- название проекта и содержание;
- цель проекта и краткое обоснование;
- подробности архитектуры и схемы;
- описание всех шагов пайплайна;
- инструкции по запуску скриптов и DAG;
- обоснование архитектурных решений;
- описание интеграции с хранилищем (облачным или локальным);
- анализ ошибок и устойчивости;
- перечень идей/предложений для развития проекта;
- скриншоты работающего дага.

- Формат сдачи: ссылка на публичный репозиторий.
- Минимальный объем: отдельные скрипты для шагов, полноценно описанный README, DAG для Airflow, сохранение выходных артефактов, пайплайн с работающей логикой.

# Дополнительно приветствуются:

- requirements.txt;
- makefile;
- использование .env для переменных;
- единый конфигурационный файл (например, config.yaml или config.py);
- архитектурные изображения или Mermaid-диаграммы в README.
