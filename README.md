# Overview

DAG bcwd pipeline:
- fetch
- import: Data wrangling
- train
  - обучения модели LogisticRegression;
  - расчета метрик (Accuracy, Precision, Recall, F1);
  - сохранения результатов (в формате JSON или CSV).
- export
  - либо в облачное хранилище (Google Drive API, Dropbox API, Amazon S3-compatible);
  - либо на локальный диск (в подкаталог results/).
  - Реализуйте авторизацию (через credentials.json, OAuth-токен или переменные окружения) и отправку файла.



# README requirements
Этап 1.
- формулировку ML-задачи;
- схематичное представление пайплайна;
- краткое описание шагов.

Этап 2.
- назначение каждого скрипта и их связи внутри пайплайна.

- fetch
  - Скачивает файл с удаленного ресурса.
  - [Скриншот](doc/img-fetch-log.png)

Этап 3.
- название DAG;
- описание зависимостей между задачами (текстом);
- инструкцию по запуску DAG (например, airflow tasks test task_id run_date).

Этап 4.
- описание интеграции (как работает, что подключается);
- место хранения ключей;
- формат данных и пример запроса/ответа или пример кода.
- Если используете локальный диск, поясните структуру хранения, формат и логику использования.

Этап 5.
- Где может «упасть» процесс?
- Какие исключения могут возникнуть?
- Что произойдет при потере соединения с источником данных?
- Что будет, если источник отдает невалидные данные?
- Что произойдет, если модель не обучается или выдает ошибку?

# TODO

etl:
- fetch
- import: Data wrangling
- train
  - обучения модели LogisticRegression;
  - расчета метрик (Accuracy, Precision, Recall, F1);
  - сохранения результатов (в формате JSON или CSV).
- export
  - либо в облачное хранилище (Google Drive API, Dropbox API, Amazon S3-compatible);
  - либо на локальный диск (в подкаталог results/).
  - Реализуйте авторизацию (через credentials.json, OAuth-токен или переменные окружения) и отправку файла.

```shell
docker build -t de-exam-dag .
```